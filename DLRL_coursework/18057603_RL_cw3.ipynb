{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_assignment_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "sC3M-IfEq2bI",
        "L9vr-iRu5pWD"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "pYs6LMEbNqoQ"
      },
      "cell_type": "markdown",
      "source": [
        "# RL homework 3\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "\n",
        "**Name:** Oliver Wesely \n",
        "\n",
        "**SN:** 18057603\n",
        "\n",
        "-----------------------------------\n",
        "\n",
        "\n",
        "**Due date:** *April 8, 2019, 9:00 am*\n",
        "\n",
        "------------------------------------\n",
        "\n",
        "## How to Submit\n",
        "\n",
        "When you have completed the exercises and everything has finsihed running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_RL_hw3.ipynb** before the deadline above."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rNuohp44N00i"
      },
      "cell_type": "markdown",
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "#### Part 1:\n",
        "You will analyse the learning dynamics of a simple MRP with linear function approximation **[30 pts]**\n",
        "\n",
        "#### Part 2:\n",
        "You will use Python to implement several reinforcement learning algorithms and you will answer a few question about the performance of these algorithms **[55pts]**.\n",
        "\n",
        "#### Part 3:\n",
        "\n",
        "Analyse optimal values and policies in a small partially observable environment **[15pts]**.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nVBcO5mAV9Ow"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Z1p0fpbxQLyn"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ps5OnkPmDbMX",
        "outputId": "246974fa-039a-4ddf-e41d-e8cd6d106941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "from collections import namedtuple\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eFnvhnKlWN_Z"
      },
      "cell_type": "markdown",
      "source": [
        "## Gridworlds"
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "e5VkDWDTWNHE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Implementation\n",
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, discount=0.9):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = np.array([\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0, 10, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "    ])\n",
        "    self._start_state = (2, 2)\n",
        "    self._goal_state = (8, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._discount = discount\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "    \n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(self._layout > -1, interpolation=\"nearest\",cmap=\"pink\")     \n",
        "    ax = plt.gca()\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(\n",
        "        self._start_state[0], self._start_state[1], \n",
        "        r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(\n",
        "        self._goal_state[0], self._goal_state[1], \n",
        "        r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
        "\n",
        "  \n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "  \n",
        "  def int_to_state(self, int_obs):\n",
        "    x = int_obs % self._layout.shape[1]\n",
        "    y = int_obs // self._layout.shape[1]\n",
        "    return y, x\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = -5.\n",
        "      discount = self._discount\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = 0.\n",
        "      discount = self._discount\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    \n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()\n",
        "  \n",
        "class AltGrid(Grid):\n",
        "  \n",
        "    def __init__(self, discount=0.9):\n",
        "      # -1: wall\n",
        "      # 0: empty, episode continues\n",
        "      # other: number indicates reward, episode will terminate\n",
        "      self._layout = np.array([\n",
        "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0, 10,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "      ])\n",
        "      self._start_state = (2, 2)\n",
        "      self._goal_state = (2, 7)\n",
        "      self._state = self._start_state\n",
        "      self._number_of_states = np.prod(np.shape(self._layout))\n",
        "      self._discount = discount\n",
        "\n",
        "class FeatureGrid(Grid):\n",
        "  \n",
        "  def get_obs(self):\n",
        "    return self.state_to_features(self._state)\n",
        "  \n",
        "  def state_to_features(self, state):\n",
        "    y, x = state\n",
        "    x /= float(self._layout.shape[1] - 1)\n",
        "    y /= float(self._layout.shape[0] - 1)\n",
        "    markers = np.arange(0.1, 1.0, 0.1)\n",
        "    features = np.array([np.exp(-40*((x - m)**2+(y - n)**2))\n",
        "                         for m in markers\n",
        "                         for n in markers] + [1.])\n",
        "    return features / np.sum(features**2)\n",
        "  \n",
        "  def int_to_features(self, int_state):\n",
        "    return self.state_to_features(self.int_to_state(int_state))\n",
        "  \n",
        "  @property\n",
        "  def number_of_features(self):\n",
        "      return len(self.get_obs())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "zV0NxnIyWVtu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Show gridworlds\n",
        "\n",
        "# Plot tabular environments\n",
        "grid = Grid()\n",
        "alt_grid = AltGrid()\n",
        "print(\"A grid world\")\n",
        "grid.plot_grid()\n",
        "plt.show()\n",
        "print(\"\\nAn alternative grid world\")\n",
        "alt_grid.plot_grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot features of each state for non tabular version of the environment.\n",
        "print(\n",
        "    \"\\nFeatures (visualised as 9x9 heatmaps) for different locations in the grid\"\n",
        "    \"\\n(Note: includes unreachable states that coincide with walls in this visualisation.)\"\n",
        ")\n",
        "feat_grid = FeatureGrid()\n",
        "shape = feat_grid._layout.shape\n",
        "f, axes = plt.subplots(shape[0], shape[1])\n",
        "for state_idx, ax in enumerate(axes.flatten()):\n",
        "  ax.imshow(np.reshape((feat_grid.int_to_features(state_idx)[:-1]),(9,9)), interpolation='nearest')\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3lpweIqAWBX3"
      },
      "cell_type": "markdown",
      "source": [
        "## Helpers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "95Ly2AaMWA5e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = env.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
        "\n",
        "    return mean_reward\n",
        "  \n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_state_value(action_values):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "def plot_action_values(action_values):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  dif = vmax - vmin\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    \n",
        "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "def parameter_study(parameter_values, parameter_name,\n",
        "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "  \n",
        "def random_policy(q):\n",
        "  return np.random.randint(4)\n",
        "\n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])\n",
        "\n",
        "def plot_greedy_policy(grid, q):\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "  greedy_actions = np.argmax(q, axis=2)\n",
        "  grid.plot_grid()\n",
        "  plt.hold('on')\n",
        "  for i in range(9):\n",
        "    for j in range(10):\n",
        "      action_name = action_names[greedy_actions[i,j]]\n",
        "      plt.text(j, i, action_name, ha='center', va='center')\n",
        "\n",
        "def plot_greedy_policy_v2(grid, pi):\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "  greedy_actions = np.argmax(pi, axis=2)\n",
        "  grid.plot_grid()\n",
        "  plt.hold('on')\n",
        "\n",
        "  h, w = grid._layout.shape\n",
        "  for y in range(2, h-2):\n",
        "    for x in range(2, w-2):\n",
        "      action_name = action_names[greedy_actions[y-2, x-2]]\n",
        "      plt.text(x, y, action_name, ha='center', va='center')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7OsOmObvq1Io"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Linear Function Approximation"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cN2i9bQ00LGn"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1\n",
        "\n",
        "We will analyze a simple Markov reward process (an MRP is an MDP without actions or, equivalently, with just 1 action in each state).  \n",
        "\n",
        "- It consists of two states. \n",
        "- The reward is zero everywhere. \n",
        "- When we are in state $s_0$, we always transition to $s_1$. \n",
        "- If we are in state $s_1$, there is a probability $p$ of terminating, after which the next episode starts in $s_0$ again.  With a probability of $1 - p$, we transition from $s_1$ back to itself again. \n",
        "- The discount is $\\gamma = 1$ on non-terminal steps.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GB21yu7I2cMb"
      },
      "cell_type": "markdown",
      "source": [
        "#### [1pt] Question 1.1\n",
        "What is the optimal value in each state?\n",
        "\n",
        "> The optimal value of each state $(s_0,s_1)$ is 0. This is the case because the rewards is zero everywhere. To see that we can just solve the bellman equationto get the optimal value by inerting 0 rewards.\n",
        "\n",
        "#### [1pt] Question 1.2\n",
        "Instead of a tabular representation, consider a single feature $\\phi$, which takes the values $\\phi(s_0) = 1$ and $\\phi(s_1) = 3$.  Now consider using linear function approximation, where we learn a value $\\theta$ such that $v_{\\theta}(s) = \\theta \\times \\phi(s) \\approx v(s)$, where $v(s)$ is the true value of state $s$.  What is the optimal value of $\\theta$?\n",
        "\n",
        ">  The optimal value of $\\theta$ is still 0, as in Question 1.1. (van Hasselt H., Doron Y., Strub F., Hessel M., Sonnerat N., Modayil J., \"Deep Reinforcement Learning and the Deadly Triad\", 2018, CoRR)\n",
        "\n",
        "### Online updates\n",
        "\n",
        "We now assume we generate data sampling from the MRP, starting each episode in state $s_0$ and following the environment's dynamics until episode termination. Updates are then performed online on each newly generated transition.\n",
        "\n",
        "#### [5pts] Question 1.3\n",
        "Suppose $\\theta_0 = 1$, and suppose we update this parameter online with TD(0) with a step size of $\\alpha = 0.1$.  What is the expected value of $\\mathbb{E}[ \\theta_T ]$ if we step through the MRP until it terminates after the first episode, as a function of $p$?  (Note that $T$ is random.)\n",
        "\n",
        "From the lecture we know our TD(0) update of the parameter $\\theta$:\n",
        "\n",
        "$\\theta_{t+1}=\\theta_t+\\alpha(R_{t+1}+\\gamma v_{\\theta}(S_{t+1})-v_{\\theta}(S_t)) \\nabla v_{\\theta}(S_t)$\n",
        "\n",
        "The first update, $s_0$ to $s_1$ is fixed and therefore the expectation ommited, as the probability is equal to 1:\n",
        "\n",
        "$\\theta_{1}=\\theta_0+\\alpha(R_{1}+\\gamma v_{\\theta}(s_{1})-v_{\\theta}(s_0)) \\nabla v_{\\theta}(s_0)$\n",
        "$\\theta_{1}=\\theta_0+\\theta_0 \\alpha(0+3-1) 1$\n",
        "$\\theta_1=\\theta_0(1+2\\alpha)$\n",
        "\n",
        "After state 1 we have two options: with probability 1-p we remain in state $s_1$ and with probability p we terminate.\n",
        "\n",
        "So we first look at the update, $s_1$ to $s_1$:\n",
        "\n",
        "$\\theta_{t+1}=\\theta_t+\\alpha(R_{t+1}+\\gamma v_{\\theta}(S_{1})-v_{\\theta}(S_{1})) \\nabla v_{\\theta}(S_1)$\n",
        "$\\theta_{t+1}=\\theta_t+\\alpha(0+\\underbrace{v_{\\theta}(s_{1})-v_{\\theta}(s_{1})}_{=0}) \\nabla v_{\\theta}(s_1)$\n",
        "$\\theta_{t+1}=\\theta_t$\n",
        "\n",
        "If it terminates, we have $\\gamma=0$, so:\n",
        "\n",
        "$\\theta_{t+1}=\\theta_t+\\alpha(0+0-3\\theta_t) 3$\n",
        "$\\theta_{t+1}=\\theta_t (1 -9\\alpha)$\n",
        "\n",
        "With these updatex we can now calculate $\\theta_T$ for some arbitrary T. So the termination is at step T, therefore:\n",
        "\n",
        "$\\theta_T=\\theta_{T-1}(1-9\\alpha)$\n",
        "\n",
        "As we have no update from state 1 to state (T-1), we know:\n",
        "\n",
        "$\\theta_T=\\theta_1(1-9\\alpha)$,\n",
        "\n",
        "finally we only have to insert our first update from state 0 to state 1:\n",
        "\n",
        "$\\theta_T=\\theta_0(1+2\\alpha)(1-9\\alpha)$\n",
        "\n",
        "With $\\theta_0=1$ and $\\alpha=0.1$ we get:\n",
        "\n",
        "$\\theta_T=1(1+2*0.1)(1-9*0.1)$\n",
        "$\\theta_T=1.2*0.1=0.12$,\n",
        "\n",
        "with probability 1.\n",
        "\n",
        "So $\\mathbb{E}[\\theta_T]=\\theta_T=0.12$\n",
        "\n",
        "#### [5pts] Question 1.4\n",
        "If $p=0.2$, how many episodes does it take, starting from $\\theta_0 = 1$, until $| v(s) - \\mathbb{E}[v_{\\theta}(s)] | < 0.1$ for all $s$, where the expectation is over the expected updates to $\\theta$?\n",
        "\n",
        "\n",
        "$\\mathbb{E}[v_{\\theta}(s)]=\\mathbb{E}[\\theta]\\mathbb{E}[\\phi(s)]=0.12 * 3$\n",
        "\n",
        "So we need to find the minimum n which satisfies $0.12^n*3<0.1$, which is:\n",
        "$n=ceiling((log(0.1/3)/log(0.12))=ceiling(1.6)=2$"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "thlNB0Mt2fee"
      },
      "cell_type": "markdown",
      "source": [
        "#### Changing the data distribution\n",
        "Now suppose we use TD to update the parameters, but instead of using the online data, we assume we can actively sample a transition from any of the two states, and on each training step we sample a transition from the first state with probability $\\beta$ or instead a transition from the second state with probability $1-\\beta$.\n",
        "\n",
        "#### [3pts] Question 1.5\n",
        "\n",
        "What is the value of $\\mathbb{E}[\\theta_n]$, as a function of $n$, $p$ and $\\beta$?\n",
        "\n",
        "\n",
        "As we sample a transition from any of the two states, we start $s_0$ with probability $\\beta$  and an expected reward of, using the solution from question 1.3:\n",
        "\n",
        "$\\mathbb{E}[v_{s_0}|S=s_0] = \\theta_0(1+2\\alpha)=1.2$\n",
        "\n",
        "Whereas looking at $s_1$ we have to consider the probability of termination or not:\n",
        "\n",
        "$\\mathbb{E}[v_{s_1}|S=s_1] = p\\theta_0(1-9\\alpha) + (1-p)\\theta_0$\n",
        "$\\mathbb{E}[v_{s_1}|S=s_1] = 0.1p +1-p=1-0.9p$\n",
        "\n",
        "We can model $\\mathbb{E}[\\theta_n]$ as a Binomial with probability $\\beta$ and n steps, following:\n",
        "\n",
        "$ \\mathbb{E}(\\theta_n) = \\sum_{k}^{n} {n \\choose k}\\beta^{k}(1.2)^{k} (1-\\beta)^{n-k}(1-0.9p)^{n-k} = \\sum_{k}^{n} {n \\choose k}(1.2\\beta)^{k} ((1-\\beta)(1-0.9p))^{n-k} $\n",
        "$ \\mathbb{E}(\\theta_n) = (1.2\\beta + (1-\\beta)(1-0.9p))^n$\n",
        "\n",
        "Reexpressing the sum by using binomial expansion properties.\n",
        "\n",
        "#### [5pts] Question 1.6\n",
        "\n",
        "When does $\\theta$ converge to the true solution? Give your answer as a function of $\\beta$ and $p$.\n",
        "\n",
        "We want $\\theta$ to converge, therefore we need to sample $\\beta \\propto p$. The proportionality exists which is all we need to obtain convergence. (Tsitsiklis J. N., van Roy B., \"An analysis of Temporal-Difference Learning with Function Approximation\", 1997, IEEE)\n",
        "\n",
        "As we already know:\n",
        "\n",
        "$ \\mathbb{E}(\\theta_n) = (1.2\\beta + (1-\\beta)(1-0.9p))^n$\n",
        "\n",
        "We know that $\\theta$ converges to 0 only when $|\\mathbb{E}[\\theta_n]|<1$.\n",
        "\n",
        "As $p,\\beta \\in [0,1]$ we get that:\n",
        "\n",
        "$1.2\\beta \\in [0,1.2]$\n",
        "$(1-\\beta)(1-0.9p) \\in [0,1]$\n",
        "\n",
        "$|\\mathbb{E}[\\theta_n]|<1$\n",
        "\n",
        "$(1.2\\beta + (1-\\beta)(1-0.9p))^n<1$\n",
        "\n",
        "$1.2\\beta + (1-\\beta)(1-0.9p)<1$\n",
        "\n",
        "$1.2\\beta + 1 -\\beta -0.9p(1-\\beta) <1$\n",
        "\n",
        "$0.2\\beta<0.9p(1-\\beta)$\n",
        "\n",
        "$\\frac{0.2\\beta}{0.9(1-\\beta)}<p$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### [5pts] Question 1.7\n",
        "\n",
        "Why doesn't it always converge?\n",
        "\n",
        "(Sutton R. S., Barto A. G., p. 215, \"Reinforcement Learning: An Introduction\") and we have off-policy training and because we are using function approximation and TD methods we  also have properties of the Deadly Triad.\n",
        "\n",
        "\n",
        "Online sampling methods which not sample frequencies natural to the Markov chain do not converge (Tsitsiklis J. N., van Roy B., \"An analysis of Temporal-Difference Learning with Function Approximation\", 1997, IEEE) And we sample our states with another distribution than we use within the MRP.\n",
        "\n",
        "\n",
        "#### [5pts] Question 1.8\n",
        "Describe one way to change the algorithm to obtain convergence of $\\theta$, for any $p$, without changing the sampling or the value function (which should remain as $v_{\\theta}(s) = \\theta \\times \\phi(s)$).  Note that the sampling is not sequential, so for instance you cannot add 'memory of the previous state', or anything like that.\n",
        "\n",
        "We only need to choose a $\\gamma$ smaller than 1, because the geometric series converges: $\\sum_{i=0}^\\infty \\gamma ^i= \\frac{1}{1-\\gamma}$, if $|\\gamma|<1$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sC3M-IfEq2bI"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Planning"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fzpb_dGVjT0O"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1: Implement Agents\n",
        "\n",
        "We are going to implement 5 agent:\n",
        "- Online Tabular Q-learning\n",
        "- Tabular Experience Replay\n",
        "- Tabular Dyna-Q (with a Tabular model)\n",
        "- Experience Replay with linear function approximation\n",
        "- Dyna-Q with linear function approximation (with a linear model)\n",
        "\n",
        "All agent you implement in this section must share the agent interface\n",
        "\n",
        "#### `__init__(self, number_of_actions, number_of_states, initial_observation)`:\n",
        "The constructor will provide the agent the number of actions, number of states, and the initial observation. You can get such initial observation by instatiating an environment (e.g., `grid = Grid()`), and then calling `grid.get_obs()`. All agents should be in pure Python - do not use TensorFlow to, e.g., compute gradients.  Using `numpy` is fine.\n",
        "\n",
        "#### `step(self, reward, discount, next_observation)`:\n",
        "The step should update the internal values, and return a new action to take. When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_observation` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_observation})$\" (for whatever definition of $v$ is appropriate) in the update, because $\\gamma = 0$.  So, the end of an episode can be seamlessly handled with the same step function. Note that to perform updates within the `step` function you typically need to store the previous state and/or action: you may set such previous action to 0 in the constructor for consumption in the first step of the first episode.\n",
        " \n",
        "#### `q_values()`:\n",
        "For tabular agents **only**. This method must return a matrix of Q values of shape: (`number_of_states`, `number_of_actions`)\n",
        "\n",
        "#### `q_values(state)`:\n",
        "For agents with function approximation **only**. This method must return an array of Q values of shape: (`number_of_actions`)\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pIgXk6LblHgV"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2.1.1\n",
        "**[2 pts]** \n",
        "\n",
        "Implement a trainable **tabular model** of the environment.\n",
        "\n",
        "The Model should implement: \n",
        "* a *next_state* method, taking a state and action and returning the next state in the environment.\n",
        "* a *reward* method, taking a state and action and returning the immediate reward associated to execution that action in that state.\n",
        "* a *discount* method, taking a state and action and returning the discount associated to execution that action in that state.\n",
        "* a *transition* method, taking a state and an action and returning both the next state and the reward associated to that transition.\n",
        "* a *update* method, taking a full transition *(state, action, reward, next_state)* and updating the model (in its reward, discount and next_state component)\n",
        "\n",
        "Given that the environment is deterministic and tabular the model will basically reduce to a simple lookup table."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "13zx3tTrll1g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TabularModel(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions):\n",
        "    pass\n",
        "\n",
        "  def next_state(self, s, a):\n",
        "    pass\n",
        "  \n",
        "  def reward(self, s, a):\n",
        "    pass\n",
        "\n",
        "  def discount(self, s, a):\n",
        "    pass\n",
        "  \n",
        "  def transition(self, state, action):\n",
        "    return (\n",
        "        self.reward(state, action), \n",
        "        self.discount(state, action),\n",
        "        self.next_state(state, action))\n",
        "  \n",
        "  def update(self, state, action, reward, discount, next_state):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rPBcz1riy_fD"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2.1.2\n",
        "\n",
        "**[3 pts]** \n",
        "\n",
        "Implement a trainable **linear model** of the environment.\n",
        "\n",
        "The Model should implement: \n",
        "* a *next_state* method, taking a state and action and returning the predicted next state in the environment.\n",
        "* a *reward* method, taking a state and action and returning the predicted immediate reward associated to execution that action in that state.\n",
        "* a *discount* method, taking a state and action and returning the predicted discount associated to execution that action in that state.\n",
        "* a *transition* method, taking a state and an action and returning both the next state and the reward associated to that transition.\n",
        "* a *update* method, taking a full transition *(state, action, reward, next_state)* and updating the model (in its reward, discount and next_state component)\n",
        "\n",
        "For each selected action, the predicted reward, discount and next state will all be a linear function of the state.\n",
        "* $\\text{s'} = T_a s$\n",
        "* $\\text{r'} = R_a s$\n",
        "* $\\text{g'} = G_a s$\n",
        "\n",
        "Where $T_a$ is a matrix of shape $(\\text{number_of_features}, \\text{number_of_features})$, $R_a$ and $G_a$are vectors of shape $(\\text{number_of_features},)$\n",
        "\n",
        "The parameters of all these linear transformations must be trained by gradient descent. Write down the update to the parameters of the models and implement the update in the model below.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "226SA-xjlyDe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LinearModel(object):\n",
        "\n",
        "  def __init__(self, number_of_features, number_of_actions):\n",
        "    pass\n",
        "\n",
        "  def next_state(self, s, a):\n",
        "    pass\n",
        "  \n",
        "  def reward(self, s, a):\n",
        "    pass\n",
        "\n",
        "  def discount(self, s, a):\n",
        "    pass\n",
        "\n",
        "  def transition(self, state, action):\n",
        "    return (\n",
        "        self.reward(state, action),\n",
        "        self.discount(state, action),\n",
        "        self.next_state(state, action))\n",
        "\n",
        "  def update(self, state, action, reward, discount, next_state, step_size=0.1):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "omzJxb5ds0Iq"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2.1.3\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Implement an agent that uses **Experience Replay** to learn action values, at each step:\n",
        "* select actions randomly\n",
        "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
        "* apply an online Q-learning update with the most recently sampled transition,\n",
        "* apply multiple Q-learning updates based on transitions sampled (uniformly) from the *replay buffer* (in addition to the online updates).\n",
        "\n",
        "So, the `step` function of the agent will, conceptually, look as follows:\n",
        "\n",
        "1. Append most recent observed transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$ to replay buffer\n",
        "\n",
        "1. Update values: $Q(S_t, A_t)$ with Q-learning, using transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$ \n",
        "\n",
        "1. Loop repeat n times:\n",
        "\n",
        "  1. Sample $S, A, R, \\gamma, S'$ from replay\n",
        "  \n",
        "  1. Update values: $Q(S, A)$ with Q-learning, using transition $(S, A, R, \\gamma, S')$ "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TB9e_reb2pJX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " class ExperienceQ(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
        "    pass\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MKfA7ifHvO-M"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Question 2.1.4\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Implement an agent that uses **Dyna-Q** to learn action values.\n",
        "* select actions randomly\n",
        "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
        "* apply an online Q-learning to Q-value\n",
        "* apply an update to the *model* based on the latest transition\n",
        "* apply multiple Q-learning updates based on transitions *(s, a, model.reward(s), model.next_state(s))* for some previous state and action pair *(s, a)*.\n",
        "\n",
        "So, the `step` function conceptually looks as follows:\n",
        "1. Append most recent observed transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$ to replay buffer\n",
        "\n",
        "1. Update values: $Q(S_t, A_t)$ with Q-learning, using transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$\n",
        "\n",
        "1. Update model: $M(S_t, A_t)$, using transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$\n",
        "\n",
        "1. Loop repeat n times:\n",
        "\n",
        "  1. Sample $S, A$ from replay\n",
        "  \n",
        "  1. Generate $R, S' = M(S, A)$\n",
        "  \n",
        "  1. Update values: $Q(S, A)$ with Q-learning, using transition $(S, A, R, \\gamma, S')$ "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WdJgVK6_3Q3-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DynaQ(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
        "    pass\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ra01mmV5VPgm"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2.1.5\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Implement an agent that uses **Experience Replay** to learn action values as a **linear function approximation** over a given set of features.\n",
        "\n",
        "Learn the value estimates via online stochastic gradient descent."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XOy_bpVa3j6V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FeatureExperienceQ(ExperienceQ):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_features, number_of_actions, *args, **kwargs):\n",
        "    super(FeatureExperienceQ, self).__init__(\n",
        "        number_of_actions=number_of_actions, *args, **kwargs)\n",
        "    pass\n",
        "\n",
        "  def q(self, state):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hlu3YPGAO9ss"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2.1.6\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Implement an agent that uses **Dyna-Q** that uses a **linear function approximation** to represent values as well as for the model of the environment.\n",
        "\n",
        "Represent and learn both the **transition model** and the **reward model** as linear, action-dependent transformations of the given set of features.  The transition and reward models should be represented separately.  Implement separate models for each action (instead of, e.g., passing a one-hot identifier of the action in).\n",
        "\n",
        "Learn value estimates, transition model and reward model via online stochastic gradient descent."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1RxFwgIU39dI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FeatureDynaQ(DynaQ):\n",
        "\n",
        "  def __init__(self, number_of_features, number_of_actions, *args, **kwargs):\n",
        "    super(FeatureDynaQ, self).__init__(\n",
        "        number_of_actions=number_of_actions, *args, **kwargs)\n",
        "    pass\n",
        "\n",
        "  def q(self, state):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1jZsPzCmDxAh"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2: Analyse Results\n",
        "\n",
        "You will have to analyse experiments that evaluate each of these 5 agents in various settings, and in terms of different metrics.\n",
        "\n",
        "- Tabular learning: data efficiency\n",
        "- Tabular learning: computational efficiency\n",
        "- Linear function approximation\n",
        "- Learning in non-stationary environments"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qekcmj4R5Y6J"
      },
      "cell_type": "markdown",
      "source": [
        "### Run data efficiency experiments\n",
        "\n",
        "*Online Q-learning*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1000$ and $\\text{num_offline_updates}$ = $0$\n",
        "\n",
        "*Experience Replay*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1000$ and $\\text{num_offline_updates}$ = $30$\n",
        "\n",
        "*DynaQ*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1000$ and $\\text{num_offline_updates}$ = $30$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Iix-yw-MKS4Y",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Online Q\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ASml5uAeIl4A",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Experience Replay\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YG-_cjw-wRzm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DynaQ\n",
        "grid = Grid()\n",
        "agent = DynaQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "teXnSHqjGfoT"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2.2.1\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "In the experiments above, how do the learnt value estimates differ between the online Q-learning, ExperienceReplay, and Dyna Q agents?\n",
        "\n",
        "Explain meaningful differences in at most 5 sentences.\n",
        "\n",
        "> *answer here*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ujZNsXFY52fi"
      },
      "cell_type": "markdown",
      "source": [
        "### Run experiments matching computational cost\n",
        "\n",
        "*Online Q-learning*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $31,000$ and $\\text{num_offline_updates}$ = $0$\n",
        "\n",
        "*ExperienceReplay*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1000$ and $\\text{num_offline_updates}$ = $30$\n",
        "\n",
        "*DynaQ*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1000$ and $\\text{num_offline_updates}$ = $30$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OVVWtGoUwiAe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# OnlineQ\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(31e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PuoUs8xVxady",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Experience Replay\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hDOW4dd221L6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DynaQ\n",
        "grid = Grid()\n",
        "agent = DynaQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nlFwZeKjLFEq"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2.2.2\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "What if sampling from the environment is cheap and we don't care about data efficiency but only care about the amount of computation we use? \n",
        "\n",
        "The experiments directly above this question are the same as those above question 2.2.1, except that we ran the experiments for the same number of **total updates**, rather than the same number of **steps in the environment**, therefore using more data for the online Q-learning algorithm which *only* updates from real data.\n",
        "\n",
        "How do the learnt values, and the relative performances, change, compared the the experiment above question 2.2.1?  Explain in at most 5 sentences.\n",
        "\n",
        "> *answer here*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GBLluo2AXMsH"
      },
      "cell_type": "markdown",
      "source": [
        "### Run experiments with linear function approximation\n",
        "\n",
        "We will now use the $\\text{FeatureGrid}$ domain, and consider the same 3 algorithms in the context of linear function approximation.\n",
        "\n",
        "*Online Q-learning*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $100,000$ and $\\text{num_offline_updates}$ = $0$\n",
        "\n",
        "*ExperienceReplay*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $100,000$ and $\\text{num_offline_updates}$ = $10$\n",
        "\n",
        "*DynaQ*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $100,000$ and $\\text{num_offline_updates}$ = $10$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zwlRPm1uXMyv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# OnlineQ\n",
        "grid = FeatureGrid()\n",
        "\n",
        "agent = FeatureExperienceQ(\n",
        "  number_of_features=grid.number_of_features, number_of_actions=4,\n",
        "  number_of_states=grid._layout.size, initial_state=grid.get_obs(),\n",
        "  num_offline_updates=0, step_size=0.01, behaviour_policy=random_policy)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = np.reshape(\n",
        "    np.array([agent.q(grid.int_to_features(i)) for i in xrange(grid.number_of_states)]),\n",
        "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wb6XeKzXcIsi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Experience Replay\n",
        "grid = FeatureGrid()\n",
        "\n",
        "agent = FeatureExperienceQ(\n",
        "  number_of_features=grid.number_of_features, number_of_actions=4,\n",
        "  number_of_states=grid._layout.size, initial_state=grid.get_obs(),\n",
        "  num_offline_updates=10, step_size=0.01, behaviour_policy=random_policy)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = np.reshape(\n",
        "    np.array([agent.q(grid.int_to_features(i)) for i in xrange(grid.number_of_states)]),\n",
        "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DVDRVknH1MXw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DynaQ\n",
        "grid = FeatureGrid()\n",
        "\n",
        "agent = FeatureDynaQ(\n",
        "  number_of_features=grid.number_of_features, \n",
        "  number_of_actions=4,\n",
        "  number_of_states=grid._layout.size, \n",
        "  initial_state=grid.get_obs(),\n",
        "  num_offline_updates=10, \n",
        "  step_size=0.01,\n",
        "  behaviour_policy=random_policy)\n",
        "\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = np.reshape(\n",
        "    np.array([agent.q(grid.int_to_features(i)) for i in xrange(grid.number_of_states)]),\n",
        "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1ese1lc0yNFU"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2.2.3\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "How do the values estimates learnt with function approximation differ from those learnt in the tabular setting, as in the experiment above question 2.2.1?\n",
        "\n",
        "Explain the results in at most 5 sentences.\n",
        "\n",
        "> *answer here*\n",
        "\n",
        "### Question 2.2.4\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Inspect the policies derived by training agents with linear function approximation on `FeatureGrid'. \n",
        "\n",
        "How do they compare to the optimal policy?\n",
        "\n",
        "> *answer here*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "arP0Nf0XUGrB"
      },
      "cell_type": "markdown",
      "source": [
        "### Run experiments in a non stationary environments\n",
        "\n",
        "We now consider a non-stationary setting where after `pretrain_steps` in the environment, the goal is moved to a new location (from the top-right of the grid to the bottom-left). The agent is allowed to continue training for a (shorter) amount of time in this new setting, and then we evaluate the value estimates.\n",
        "\n",
        "*Online Q-learning*\n",
        "\n",
        "* $\\text{pretrain_steps}$ = $20,000$,  $\\text{num_steps}$ = $666$, and $\\text{num_offline_updates}$ = $0$\n",
        "\n",
        "*ExperienceReplay*\n",
        "\n",
        "* $\\text{pretrain_steps}$ = $20,000$,  $\\text{num_steps}$ = $666$ and $\\text{num_offline_updates}$ = $10$\n",
        "\n",
        "*DynaQ*\n",
        "\n",
        "* $\\text{pretrain_steps}$ = $20,000$,  $\\text{num_steps}$ = $666$ and $\\text{num_offline_updates}$ = $10$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6P9lC323X7uH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Online Q\n",
        "\n",
        "# Train on first environment\n",
        "pretrain_steps = 2e4\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(pretrain_steps))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# Change goal location\n",
        "alt_grid = AltGrid()\n",
        "run_experiment(alt_grid, agent, int(pretrain_steps / 30))\n",
        "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
        "plot_state_value(alt_q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bq5msw1iY-Q5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Experience Replay\n",
        "\n",
        "# Train on first environment\n",
        "pretrain_steps = 2e4\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(pretrain_steps))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# Change goal location\n",
        "alt_grid = AltGrid()\n",
        "run_experiment(alt_grid, agent, int(pretrain_steps / 30))\n",
        "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
        "plot_state_value(alt_q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AwztU4EbUXe0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DynaQ\n",
        "\n",
        "# Train on first environment\n",
        "pretrain_steps = 2e4\n",
        "grid = Grid()\n",
        "agent = DynaQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(pretrain_steps))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# Change goal location\n",
        "alt_grid = AltGrid()\n",
        "run_experiment(alt_grid, agent, int(pretrain_steps / 30))\n",
        "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
        "plot_state_value(alt_q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lc8tJPpXyNM7"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2.2.5\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Compare the value estimates of online Q-learning and Dyna-Q, after training also on the new goal location.\n",
        "\n",
        "Explain what you see in at most 5 sentences. \n",
        "\n",
        "> *answer here*\n",
        "\n",
        "### Question 2.2.6\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Compare the value estimates of online Experience Replay and Dyna-Q, after training also on the new goal location, explain what you see.\n",
        "\n",
        "> *answer here*\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L9vr-iRu5pWD"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3: Policy Gradients"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "iIRC73HLq6VH"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1. Policy Gradients"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3GC9gSqYrJfB"
      },
      "cell_type": "markdown",
      "source": [
        "Consider a simple 2x2 gridworld.\n",
        "\n",
        "- The agent starts in one of the top cells.\n",
        "- Both cells on the bottom row are terminal.\n",
        "- The bottom left cell provides a negative -1 reward.\n",
        "- The bottom right cell provides a positive +1 reward.\n",
        "- There is a fixed discount of 0.9\n"
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "6EB_E5npX9KM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title MDP rewards\n",
        "\n",
        "plt.figure(figsize=(3,0.7))\n",
        "clust_data = np.array([[0, 0], [-1, 1]])\n",
        "collabel=(\"col 1\", \"col 2\", \"col 3\")\n",
        "the_table = plt.table(cellText=clust_data, loc='center')\n",
        "plt.axis('tight')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Qf8XzXLbtW3K"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 3.1.1\n",
        "\n",
        "**[1 pts]**\n",
        "\n",
        "What are the *action values*  if the agent can perceive exactly in what state it is?\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "### Question 3.1.2\n",
        "\n",
        "**[4 pts]**\n",
        "\n",
        "What are the *action values*  if the agent cannot tell whether he is in the right or left column?\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "### Question 3.1.3\n",
        "\n",
        "**[10 pts]**\n",
        "\n",
        "What is the optimal policy if the agent cannot tell whether he is in the right or left column??\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WO5bV7Fqrmgm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}